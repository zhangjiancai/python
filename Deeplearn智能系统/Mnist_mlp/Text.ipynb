{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "介绍如何用 Python 语言实现基于VGG19的图像分类\n",
    "复用第2章实验中的全连接层、ReLU 激活函数层 等基本单元，在此基础上加入新的基本单元如卷积层、最大池化层。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#广播运算\n",
    "import numpy as np\n",
    "x = np.array(range(12)).reshape(3,4)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "x1=x + 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "x2 = np.array([1,2,3,4])\n",
    "x3=x + x2"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "基于三层神经网络实现手写数字分类"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "DataSet:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_mnist(self,file_dir,is_images = 'True'):\n",
    "    bin_file = open(file_dir,'rb')\n",
    "    bin_data = bin_file.read()\n",
    "    bin_file.close()\n",
    "    if is_images: #read the image data \n",
    "        fmt_header = '>iiii'\n",
    "        magic,num_images,num_rows,num_cols = struct.unpack_from(fmt_header,bin_data,0)\n",
    "    else: #read label data\n",
    "        fmt_header = '>ii'\n",
    "        magic,num_images = struct.unpack_from(fmt_header,bin_data,0)\n",
    "        num_rows,num_cols = 1 , 1 \n",
    "    data_size = num_images * num_rows * num_cols\n",
    "    mat_data = struct,unpack_from('>' + str(data_size) + 'B',bin_data,struct.calcsize(fmt_header))\n",
    "    mat_data = np.reshape(map_data,[num_images,num_row * num_cols])\n",
    "    return mat_data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "子数据集的读取和预处理"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_data(self):\n",
    "    # TODO: 调用函数 load_mnist 读取和预处理 MNIST 中训练数据和测试数据的图像和标记\n",
    "    print('Loading MNIST data from files...（使用了load_mnist函数）')\n",
    "    train_images = self.load_mnist(os.path.join(MNIST_DIR,TRAIN_DATA),True)\n",
    "    train_labels = self.load_mnist(os.path.join(MNIST_DIR,TRAIN_lABEL),False)\n",
    "    \n",
    "    test_images = self.load_mnist(os.path.join(MNIST_DIR, TEST_DATA), True)\n",
    "    test_labels = self.load_mnist(os.path.join(MNIST_DIR, TEST_LABEL), False)\n",
    "\n",
    "    self.train_data = np.append(train_images, train_labels, axis=1)\n",
    "    self.test_data = np.append(test_images, test_labels, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def shuffle_data(self):\n",
    "    print('Randomly shuffle MNIST data...')\n",
    "    np.random.shuffle(self.train_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    ":T: X :T:   the usage of axis in append "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "axis = 0 时，数据加在列方向"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0. 0. 0.]\n",
      " [0. 0. 0.]\n",
      " [0. 0. 0.]\n",
      " [1. 1. 1.]\n",
      " [1. 1. 1.]\n",
      " [1. 1. 1.]]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "aa= np.zeros((3,3))\n",
    "bb=np.ones((3,3))\n",
    "c = np.append(aa,bb,axis = 0)\n",
    "print(c)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "axis = 1 时，数据加在行反向"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0. 0. 0. 1. 1. 1.]\n",
      " [0. 0. 0. 1. 1. 1.]\n",
      " [0. 0. 0. 1. 1. 1.]]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "aa= np.zeros((3,3))\n",
    "bb=np.ones((3,3))\n",
    "c = np.append(aa,bb,axis = 1)\n",
    "print(c)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "不定义axis时，数据为一维"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 1. 1. 1.]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "aa= np.zeros((3,8))\n",
    "bb=np.ones((3,1))\n",
    "c = np.append(aa,bb)\n",
    "print(c)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "通常同类型的层用一个类来定义，类的成员函数通常包括层的初始化、参数的初 始化、前向传播计算、反向传播计算、参数的更新、参数的加载和保存等"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class FullyConnectedLayer(object):\n",
    "\n",
    "    def __init__(self, num_input, num_output):  # 全连接层初始化\n",
    "        self.num_input = num_input\n",
    "        self.num_output = num_output\n",
    "        print('\\tFully connected layer with input %d, output %d.' % (self.num_input, self.num_output))\n",
    "    \n",
    "    def init_param(self, std=0.01):  # 参数初始化\n",
    "        self.weight = np.random.normal(loc=0.0, scale=std, size=(self.num_input, self.num_output))\n",
    "        self.bias = np.zeros([1, self.num_output])\n",
    "\n",
    "    def forward(self, input):  # 前向传播计算\n",
    "        start_time = time.time()\n",
    "        print('\\t开始记时:')\n",
    "        self.input = input\n",
    "        # TODO：全连接层的前向传播，计算输出结果\n",
    "        #Y = XW + b (2.3)\n",
    "        self.output = np.dot(self.input, self.weight) + self.bias\n",
    "        return self.output      \n",
    "#np.dot可以支持矩阵相乘。\n",
    "##所以下面的反向传播使用的是dot，因为我们通常使用梯度下降法，\n",
    "##来进行反向梯度下降。选择若干样本同时计算。\n",
    "##输入就变成了二维矩阵了。\n",
    "\n",
    "## input , weight ,bias ; 依次对应：x, w ,b ---OK \n",
    "\n",
    "    def backward(self, top_diff):  # 反向传播的计算\n",
    "        # TODO：全连接层的反向传播，计算参数梯度和本层损失\n",
    "        #(2.4)self.input.T 是input的转置，top_diff 被称为损失函数对上一层的偏导。\n",
    "        self.d_weight = np.dot(self.input.T, top_diff)\n",
    "\n",
    "    #d_weight表示此计算层的权重的梯度；下面的d_bias同理。\n",
    "    #注意，此时反向传播的下一层是正向传播的上一层。\n",
    "        \n",
    "        self.batch_size = top_diff.shape[0]\n",
    "\n",
    "    #shape的功能是读取矩阵的长度，比如shape[0]就是读取矩阵第一维度的长度\n",
    "    ##batch_size 为下面的d_bias的维度做个扩展，一般来说与广播计算类似\n",
    "        \n",
    "        self.d_bias = np.dot(np.ones(shape=(1,self.batch_size)),top_diff)\n",
    "        \n",
    "        bottom_diff = np.dot(top_diff, self.weight.T)\n",
    "        return bottom_diff\n",
    "\n",
    "    def update_param(self, lr):  # 参数更新\n",
    "        # TODO：对全连接层参数利用参数进行更新\n",
    "\n",
    "    ##lr学习率，确定每次更新时的幅度。\n",
    "        \n",
    "        self.weight = self.weight - lr * self.d_weight\n",
    "        self.bias = self.bias - lr * self.d_bias\n",
    "    def load_param(self, weight, bias):  # 参数加载\n",
    "        assert self.weight.shape == weight.shape\n",
    "        assert self.bias.shape == bias.shape\n",
    "        self.weight = weight\n",
    "        self.bias = bias\n",
    "    def save_param(self):  # 参数保存\n",
    "        return self.weight, self.bias"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.11.0 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "5238573367df39f7286bb46f9ff5f08f63a01a80960060ce41e3c79b190280fa"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
